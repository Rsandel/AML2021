{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import fnmatch\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, LeakyReLU, Input, GRU\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "import random\n",
    "\n",
    "import re\n",
    "\n",
    "##################\n",
    "# Default tick label size\n",
    "plt.rcParams['xtick.labelsize'] = 24\n",
    "plt.rcParams['ytick.labelsize'] = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_sequence(X, min_delay=1, max_delay=1):\n",
    "    '''\n",
    "    Generate a single sequence given a vectorial (column) cue\n",
    "    \n",
    "    The general sequence is:\n",
    "    - CUE: 1 step\n",
    "    - Wait with CUE: random number of steps\n",
    "    - Movement with no CUE: 3 steps\n",
    "    - Hold with no CUE: random number of steps\n",
    "    '''\n",
    "    # Initial state\n",
    "    cue = X\n",
    "    go = np.array([[0.0]])\n",
    "    \n",
    "    ins = np.vstack((cue, go))\n",
    "    outs = np.zeros((X.shape[0], 1))\n",
    "    \n",
    "    # Wait period\n",
    "    n = random.randint(min_delay, max_delay)+1\n",
    "    \n",
    "    # Set cue to zero (force network to remember)\n",
    "    cue = np.zeros(cue.shape)\n",
    "    \n",
    "    # Loop over wait  period\n",
    "    for i in range(n):\n",
    "        ins_new = np.vstack((cue, go))\n",
    "        outs_new = np.zeros((X.shape[0], 1))\n",
    "        ins = np.hstack((ins, ins_new))\n",
    "        outs = np.hstack((outs, outs_new))\n",
    "\n",
    "    # Go: move step 1\n",
    "    cue = np.zeros((cue.shape[0], 1))\n",
    "    go = np.array([[1.0]])\n",
    "    ins_new = np.vstack((cue, go))\n",
    "    outs_new = X * 0.33\n",
    "    ins = np.hstack((ins, ins_new))\n",
    "    outs = np.hstack((outs, outs_new))\n",
    "    \n",
    "    # Move step 2\n",
    "    go = np.array([[1.0]])\n",
    "    ins_new = np.vstack((cue, go))\n",
    "    outs_new = X * 0.67\n",
    "    ins = np.hstack((ins, ins_new))\n",
    "    outs = np.hstack((outs, outs_new))\n",
    "    \n",
    "    # Move step 3\n",
    "    go = np.array([[1.0]])\n",
    "    ins_new = np.vstack((cue, go))\n",
    "    outs_new = X \n",
    "    ins = np.hstack((ins, ins_new))\n",
    "    outs = np.hstack((outs, outs_new))\n",
    "    \n",
    "    # Repeat final step\n",
    "    for i in range(10-n):\n",
    "        ins = np.hstack((ins, ins_new))\n",
    "        outs = np.hstack((outs, outs_new))\n",
    "        \n",
    "    return ins, outs\n",
    "\n",
    "def generate_sequence_set_from_cues(cues, min_delay=1, max_delay=1):\n",
    "    '''\n",
    "    Generate a set of sequences\n",
    "    \n",
    "    Output format: SEQUENCES x STEPS x FEATURES\n",
    "    '''\n",
    "    \n",
    "    # First one\n",
    "    cue = np.array([[cues[0][0]],[cues[0][1]]])\n",
    "    ins_single, outs_single = generate_sequence(cue, min_delay=min_delay, max_delay=max_delay)\n",
    "    ins_single = ins_single.T\n",
    "    outs_single = outs_single.T\n",
    "    \n",
    "    # Shape of a single trial\n",
    "    ins_shape = ins_single.shape\n",
    "    outs_shape = outs_single.shape\n",
    "    \n",
    "    # Change to 1 x STEPS x features\n",
    "    ins = ins_single.reshape((1,ins_shape[0], ins_shape[1]))\n",
    "    outs = outs_single.reshape((1,outs_shape[0], outs_shape[1]))\n",
    "    \n",
    "    ##################\n",
    "    # Add the others\n",
    "    for cue in cues[1:]:\n",
    "        cue = np.array([[cue[0]],[cue[1]]])\n",
    "        ins_single, outs_single = generate_sequence(cue, min_delay=min_delay, max_delay=max_delay)\n",
    "        ins = np.insert(ins, ins.shape[0], ins_single.T, axis=0)\n",
    "        outs = np.insert(outs, outs.shape[0], outs_single.T, axis=0)\n",
    "    \n",
    "    return ins, outs\n",
    "\n",
    "def generate_full_sequence_set(min_delay=1, max_delay=6):\n",
    "    '''\n",
    "    Generate a set of sequences from a fixed set of goals\n",
    "    \n",
    "    Output format: SEQUENCES x STEPS x FEATURES\n",
    "    '''\n",
    "    \n",
    "    cues = [[1, 0], [-1, 0], [0, 1], [0, -1],\n",
    "            [1, 1], [-1, 1], [-1, 1], [-1, -1],\n",
    "            [1, 0], [-1, 0], [0, 1], [0, -1],\n",
    "            [1, 1], [-1, 1], [-1, 1], [-1, -1],\n",
    "            [1, 0], [-1, 0], [0, 1], [0, -1],\n",
    "            [1, 1], [-1, 1], [-1, 1], [-1, -1],\n",
    "            [1, 0], [-1, 0], [0, 1], [0, -1],\n",
    "            [1, 1], [-1, 1], [-1, 1], [-1, -1]]\n",
    "    return generate_sequence_set_from_cues(cues, min_delay, max_delay)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training and validation sets\n",
    "ins, outs = generate_full_sequence_set()\n",
    "ins_validation, outs_validation = generate_full_sequence_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric_leaky_relu(X):\n",
    "    '''\n",
    "    This non-linearity \"leaks\" on both the positive and negative sides\n",
    "    \n",
    "    :param X: net input tensor\n",
    "    :return: tensor that contains the output of the non-linearity\n",
    "    '''\n",
    "    return 1.0-LeakyReLU(0.1)(1.0-LeakyReLU(0.1)(X))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(ins, outs, n_neurons=10, activation='tanh', \n",
    "                   activation_dense=None, lambda_regularization=0):\n",
    "    ins_shape = ins.shape\n",
    "    outs_shape = outs.shape\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Fill in\n",
    "    \n",
    "    \n",
    "    model.add(Dense(units=int(n_neurons/2), activation=activation_dense,\n",
    "                         kernel_initializer='random_uniform',\n",
    "                           bias_initializer='random_uniform',\n",
    "                            kernel_regularizer=keras.regularizers.l2(lambda_regularization),\n",
    "                            bias_regularizer=keras.regularizers.l2(lambda_regularization)))\n",
    "    \n",
    "    model.add(Dense(units=2,\n",
    "                   kernel_initializer='random_uniform',\n",
    "                           bias_initializer='random_uniform',\n",
    "                            kernel_regularizer=keras.regularizers.l2(lambda_regularization),\n",
    "                            bias_regularizer=keras.regularizers.l2(lambda_regularization)))\n",
    "    \n",
    "              \n",
    "    # The optimizer determines how the gradient descent is to be done\n",
    "    opt = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, \n",
    "                            epsilon=None, decay=0.0, amsgrad=False)\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=opt)\n",
    "              \n",
    "    return model\n",
    "\n",
    "def create_network_gru(ins, outs, n_neurons=10, activation='tanh', activation_dense=None, lambda_regularization=0):\n",
    "    ins_shape = ins.shape\n",
    "    outs_shape = outs.shape\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Fill in\n",
    "    \n",
    "    model.add(Dense(units=int(n_neurons/2), activation=activation_dense,\n",
    "                         kernel_initializer='random_uniform',\n",
    "                           bias_initializer='random_uniform',\n",
    "                            kernel_regularizer=keras.regularizers.l2(lambda_regularization),\n",
    "                            bias_regularizer=keras.regularizers.l2(lambda_regularization)))\n",
    "    \n",
    "    model.add(Dense(units=2,\n",
    "                   kernel_initializer='random_uniform',\n",
    "                           bias_initializer='random_uniform',\n",
    "                            kernel_regularizer=keras.regularizers.l2(lambda_regularization),\n",
    "                            bias_regularizer=keras.regularizers.l2(lambda_regularization)))\n",
    "    \n",
    "              \n",
    "    # The optimizer determines how the gradient descent is to be done\n",
    "    opt = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, \n",
    "                            epsilon=None, decay=0.0, amsgrad=False)\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=opt)\n",
    "              \n",
    "    return model\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_network(model, ins, outs, \n",
    "                  performance_log=[], sample=0):\n",
    "    pred = model.predict(ins)\n",
    "    \n",
    "    fs =18\n",
    "    # Plot sample\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    \n",
    "    # X\n",
    "    plt.subplot(121)\n",
    "    plt.plot(pred[sample,:,0], 'b.')\n",
    "    plt.plot(outs[sample,:,0], 'r.')\n",
    "    plt.ylim(-1.2,1.2)\n",
    "    plt.title(\"X\", fontsize=fs)\n",
    "    plt.ylabel(\"position\", fontsize=fs)\n",
    "    \n",
    "    # Y\n",
    "    plt.subplot(122)\n",
    "    plt.plot(pred[sample,:,1], 'b.')\n",
    "    plt.plot(outs[sample,:,1], 'r.')\n",
    "    plt.ylim(-1.2,1.2)\n",
    "    plt.title(\"Y\", fontsize=fs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_network(ins, outs, n_neurons=40, \n",
    "                       activation=symmetric_leaky_relu, \n",
    "                       activation_dense=symmetric_leaky_relu, \n",
    "                       lambda_regularization=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(ins, outs, epochs=1000, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_network(model, ins_validation, outs_validation)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = create_network_gru(ins, outs, n_neurons=40, \n",
    "                       activation_dense=symmetric_leaky_relu, lambda_regularization=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_gru.fit(ins, outs, epochs=1000, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_network(model_gru, ins_validation, outs_validation)\n",
    "print(model_gru.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we want to clear out all of the older models\n",
    "backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
